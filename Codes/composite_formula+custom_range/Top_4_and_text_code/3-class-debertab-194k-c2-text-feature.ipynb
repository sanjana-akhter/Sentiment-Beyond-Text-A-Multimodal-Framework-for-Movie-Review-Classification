{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13876591,"sourceType":"datasetVersion","datasetId":8841021}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# --- INSTALLING LIBRARIES ---\n!pip install --upgrade transformers accelerate datasets scikit-learn -q\n\n# --- IMPORTING LIBRARIES ---\nimport pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom transformers import AutoTokenizer, DebertaV2Model, TrainingArguments, Trainer \nfrom datasets import Dataset, DatasetDict\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report, confusion_matrix\nfrom sklearn.preprocessing import StandardScaler, MultiLabelBinarizer, OneHotEncoder\nfrom sklearn.utils.class_weight import compute_class_weight \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport shutil # Import for Part 5\n\nRNG = 42\n\n# --- LOADING THE DATASET ---\n\n# âœ… FIXED PATH BELOW\nFILE_PATH = '/kaggle/input/composite-1-194k-data-deberta-text/Composite2_194k data.csv' \n\nprint(f\"Loading dataset from: {FILE_PATH}\")\n\ntry:\n    df = pd.read_csv(FILE_PATH)\nexcept FileNotFoundError:\n    print(\"\\nâŒ ERROR: File not found!\")\n    print(f\"Please verify the path: {FILE_PATH}\")\n    print(\"Tip: In Kaggle, check the 'Input' section on the right sidebar to copy the exact path.\")\n    raise\n\n# --- SANITY CHECK ---\nneeded_cols = [\n    'title', 'reviewText', 'audienceScore', 'tomatoMeter', 'runtimeMinutes',\n    'genre', 'language_encoded', 'director_encoded', 'target_label'\n]\n\nmissing = [c for c in needed_cols if c not in df.columns]\nif missing:\n    raise ValueError(f\"âŒ CRITICAL ERROR: Your CSV is missing these columns: {missing}\")\n\nprint(\"--- Part 1: Setup and Data Loading Complete (Kaggle Version) ---\")\nprint(f\"Dataset loaded with {df.shape[0]} rows.\")\nprint(f\"Target Distribution:\\n{df['target_label'].value_counts().sort_index()}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T09:15:03.550595Z","iopub.execute_input":"2025-11-26T09:15:03.550979Z","iopub.status.idle":"2025-11-26T09:17:01.796012Z","shell.execute_reply.started":"2025-11-26T09:15:03.550949Z","shell.execute_reply":"2025-11-26T09:17:01.795312Z"},"editable":false},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m99.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m380.9/380.9 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m110.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m91.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m105.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m81.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m47.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m92.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\npylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ncudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ncategory-encoders 2.7.0 requires scikit-learn<1.6.0,>=1.0.0, but you have scikit-learn 1.7.2 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nsklearn-compat 0.1.3 requires scikit-learn<1.7,>=1.2, but you have scikit-learn 1.7.2 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\ncudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"},{"name":"stderr","text":"2025-11-26 09:16:38.370799: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1764148598.576904      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1764148598.639961      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"name":"stdout","text":"Loading dataset from: /kaggle/input/composite-1-194k-data-deberta-text/Composite2_194k data.csv\n--- Part 1: Setup and Data Loading Complete (Kaggle Version) ---\nDataset loaded with 194784 rows.\nTarget Distribution:\ntarget_label\n0    55029\n1    66081\n2    73674\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# --- PART 2: PREPROCESSING & FEATURE ENGINEERING ---\n\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom packaging import version\nimport sklearn\n\n# Ensure 'df' and 'RNG' exist from Part 1\nif 'df' not in globals():\n    raise ValueError(\"âŒ 'df' is missing! Please run Part 1 to load the data first.\")\n\n# --- CLEANING ---\nprint(\"Cleaning data...\")\ndf = df.dropna(subset=['reviewText', 'target_label']).copy()\n# Map 'target_label' to the standard 'label' column expected by HF\ndf['label'] = df['target_label'].astype(int)\n\nclass_names = ['Negative', 'Neutral', 'Positive']\nN_CLASSES = 3\n\n# Text Prep\n# ğŸš¨ CRITICAL CHANGE: Only using reviewText for the combined_text field ğŸš¨\ndf['combined_text'] = df['reviewText']\n\n# --- STRATIFIED SPLIT ---\nprint(\"Splitting data (Stratified)...\")\ntrain_idx, test_idx = train_test_split(\n    df.index, test_size=0.2, random_state=RNG, stratify=df['label']\n)\ntrain_df = df.loc[train_idx].reset_index(drop=True)\ntest_df  = df.loc[test_idx].reset_index(drop=True)\n\ny_train = train_df['label'].values.astype(int)\n\n# ==========================================\n# CRITICAL: CALCULATE CLASS WEIGHTS\n# ==========================================\nprint(\"Calculating class weights...\")\n# This forces the model to pay attention to minority classes\nclass_weights = compute_class_weight(\n    class_weight=\"balanced\",\n    classes=np.unique(y_train),\n    y=y_train\n)\nclass_weights = torch.tensor(class_weights, dtype=torch.float)\nprint(\"\\nâš–ï¸ Calculated Class Weights (Neg, Neu, Pos):\")\nprint(class_weights)\nprint(\"(The model will use these to penalize mistakes on smaller classes more heavily)\")\n\n# ==========================================\n# FEATURE ENGINEERING (Text-Only)\n# ğŸš¨ CRITICAL CHANGE: Removing ALL Non-Text Feature Processing ğŸš¨\n# Setting placeholders (Dummy Scalers/MLB) to ensure Part 3 runs smoothly \n# by returning zero-dimension arrays for non-text features.\n# ==========================================\nprint(\"Starting Feature Engineering (Text-Only: ReviewText)...\")\n\n# Placeholder definitions for missing features to prevent downstream errors\nNUMERIC_COLS = []\ntrain_medians = pd.Series()\n\n# Create dummy objects that return empty arrays for numeric/categorical features\nscaler = type('DummyScaler', (object,), {'transform': lambda self, x: np.zeros((len(x), 0), dtype=np.float32)})()\nmlb = type('DummyMLB', (object,), {'transform': lambda self, x: np.zeros((len(x), 0), dtype=np.float32)})()\n# Dummy variables needed for Part 3's required_vars check\ndir_train_block = np.zeros((len(train_df), 0), dtype=np.float32)\ndir_test_block  = np.zeros((len(test_df), 0), dtype=np.float32) \n\nprint(\"--- Part 2: Preprocessing & Weights Complete (Text-Only) ---\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T09:17:08.157987Z","iopub.execute_input":"2025-11-26T09:17:08.158270Z","iopub.status.idle":"2025-11-26T09:17:08.456801Z","shell.execute_reply.started":"2025-11-26T09:17:08.158242Z","shell.execute_reply":"2025-11-26T09:17:08.456121Z"},"editable":false},"outputs":[{"name":"stdout","text":"Cleaning data...\nSplitting data (Stratified)...\nCalculating class weights...\n\nâš–ï¸ Calculated Class Weights (Neg, Neu, Pos):\ntensor([1.1799, 0.9825, 0.8813])\n(The model will use these to penalize mistakes on smaller classes more heavily)\nStarting Feature Engineering (Text-Only: ReviewText)...\n--- Part 2: Preprocessing & Weights Complete (Text-Only) ---\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# --- PART 3: DATASET CREATION & TOKENIZATION ---\n\nimport numpy as np\nfrom datasets import Dataset, DatasetDict\nfrom transformers import AutoTokenizer\n\n# Ensure prerequisites from Part 2 exist\nrequired_vars = ['train_df', 'test_df', 'dir_train_block', 'dir_test_block', 'scaler', 'mlb', 'train_medians']\nif not all(v in globals() for v in required_vars):\n    raise ValueError(\"âŒ Missing variables from Part 2. Please run the previous cell first.\")\n\n# Define helper function\ndef build_features(split_df):\n    out = {}\n    out['combined_text'] = split_df['combined_text'].tolist()\n    out['label'] = split_df['label'].astype(int).tolist()\n\n    # Numeric (Will be zero-dimension via DummyScaler)\n    # We pass a column just to get the row count for the zero-array output\n    numeric = split_df[['tomatoMeter']].copy().fillna(train_medians) \n    out['numerical_features'] = scaler.transform(numeric.values).astype(np.float32)\n\n    # Categorical (Will be zero-dimension via DummyMLB)\n    genre_list = [[]] * len(split_df)\n    out['categorical_features'] = mlb.transform(genre_list).astype(np.float32)\n    \n    return out\n\nprint(\"Building features dictionaries (Text-Only)...\")\ntrain_feats = build_features(train_df)\ntest_feats  = build_features(test_df)\n\n# Dimensions\n# ğŸš¨ CRITICAL: Set feature counts to 0 ğŸš¨\nnum_numerical_features = 0\nnum_categorical_features = 0\nprint(f\"Feature Dims: Numeric={num_numerical_features}, Categorical={num_categorical_features}\")\n\n# HF Dataset\nprint(\"Converting to Hugging Face Datasets...\")\nraw_datasets = DatasetDict({\n    'train': Dataset.from_dict(train_feats),\n    'test':  Dataset.from_dict(test_feats),\n})\n\n# Tokenization\nMODEL_CHECKPOINT = \"microsoft/deberta-v3-base\" \n\nprint(f\"Downloading Tokenizer ({MODEL_CHECKPOINT})...\")\ntry:\n    tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT) \nexcept Exception as e:\n    print(\"\\nâŒ ERROR: Could not download tokenizer.\")\n    print(\"Please check if 'Internet' is enabled in the Kaggle Notebook settings (Right Sidebar > Settings > Internet 'On').\")\n    raise e\n\ndef tokenize_fn(examples):\n    # DeBERTa models often use max_length=256 in fine-tuning examples\n    return tokenizer(examples[\"combined_text\"], padding=\"max_length\", truncation=True, max_length=256)\n\nprint(\"Tokenizing dataset (this may take a moment)...\")\ntokenized_datasets = raw_datasets.map(tokenize_fn, batched=True)\ntokenized_datasets = tokenized_datasets.remove_columns([\"combined_text\"])\ntokenized_datasets.set_format(\"torch\")\n\nprint(f\"--- Part 3: Dataset Ready & Tokenized (Feature Dims: Num={num_numerical_features}, Cat={num_categorical_features}) ---\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T09:17:12.833677Z","iopub.execute_input":"2025-11-26T09:17:12.833966Z","iopub.status.idle":"2025-11-26T09:17:51.282209Z","shell.execute_reply.started":"2025-11-26T09:17:12.833945Z","shell.execute_reply":"2025-11-26T09:17:51.281601Z"},"editable":false},"outputs":[{"name":"stdout","text":"Building features dictionaries (Text-Only)...\nFeature Dims: Numeric=0, Categorical=0\nConverting to Hugging Face Datasets...\nDownloading Tokenizer (microsoft/deberta-v3-base)...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e961b7864bd24f8c8020c77333863f3b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/579 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9569241e82d141b18cfdd6f1c5a4580e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c33f9af028374d698b33d598aa503dfa"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:566: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Tokenizing dataset (this may take a moment)...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/155827 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fee27479900f47758783853a2590d8fb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/38957 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a6412e6442b48af95bb5790f550a7d7"}},"metadata":{}},{"name":"stdout","text":"--- Part 3: Dataset Ready & Tokenized (Feature Dims: Num=0, Cat=0) ---\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# --- PART 4: MODEL SETUP & TRAINING (KAGGLE VERSION - FIXED) ---\n\nimport torch\nimport torch.nn as nn\nfrom transformers import Trainer, TrainingArguments, DebertaV2Model \nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\nimport numpy as np\nimport os\n\n# --- SAFETY CHECK ---\nif 'class_weights' not in globals():\n    raise ValueError(\"âŒ 'class_weights' is missing! Please run Part 2 first.\")\nif 'MODEL_CHECKPOINT' not in globals() or 'num_numerical_features' not in globals() or 'num_categorical_features' not in globals():\n    raise ValueError(\"âŒ Model checkpoint or feature dimensions missing! Please run Part 3 first.\")\n\n\n# --- MODEL ARCHITECTURE ---\nclass MultimodalClassifier(nn.Module):\n    def __init__(self, num_labels, num_numerical, num_categorical):\n        super().__init__()\n        self.deberta = DebertaV2Model.from_pretrained(MODEL_CHECKPOINT) \n        self.dropout = nn.Dropout(0.2)\n        # ğŸš¨ CRITICAL CHANGE: Input size is DeBERTa output (768) + 0 + 0 ğŸš¨\n        self.classifier = nn.Linear(\n            self.deberta.config.hidden_size + num_numerical + num_categorical,\n            num_labels\n        )\n        self.num_labels = num_labels\n\n    def forward(self, input_ids, attention_mask, numerical_features, categorical_features, labels=None):\n        deberta_output = self.deberta(input_ids=input_ids, attention_mask=attention_mask)\n        pooled_output = self.dropout(deberta_output.last_hidden_state[:, 0])\n\n        # Combined vector is now just pooled_output\n        combined = torch.cat([pooled_output, numerical_features, categorical_features], dim=1)\n        logits = self.classifier(combined)\n\n        loss = None\n        if labels is not None:\n            loss_fct = nn.CrossEntropyLoss()\n            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n\n        return (loss, logits) if loss is not None else (None, logits)\n\n# --- CUSTOM TRAINER (FIXED FOR DATAPARALLEL) ---\nclass WeightedTrainer(Trainer):\n    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n        labels = inputs.get(\"labels\")\n        outputs = model(**inputs)\n        logits = outputs[1]\n\n        if hasattr(model, \"module\"):\n            classifier_layer = model.module.classifier\n        else:\n            classifier_layer = model.classifier\n\n        # Move weights to the correct device\n        weights = class_weights.to(classifier_layer.weight.device)\n\n        # Weighted Loss Calculation\n        loss_fct = nn.CrossEntropyLoss(weight=weights)\n        loss = loss_fct(logits.view(-1, self.model.num_labels), labels.view(-1))\n\n        return (loss, outputs) if return_outputs else loss\n\n# --- DATA COLLATOR, METRICS, INITIALIZATION (No Change) ---\\n\nclass MultimodalDataCollator:\n    def __init__(self, tokenizer):\n        self.tokenizer = tokenizer\n    def __call__(self, features):\n        batch = self.tokenizer.pad(\n            [{\"input_ids\": f[\"input_ids\"], \"attention_mask\": f[\"attention_mask\"]} for f in features],\n            return_tensors=\"pt\"\n        )\n        batch['labels'] = torch.tensor([f['label'] for f in features], dtype=torch.long)\n        batch['numerical_features'] = torch.stack([f['numerical_features'] for f in features])\n        batch['categorical_features'] = torch.stack([f['categorical_features'] for f in features])\n        return batch\n\ndef compute_metrics(pred):\n    labels = pred.label_ids\n    \n    if isinstance(pred.predictions, tuple):\n        predictions = pred.predictions[0]\n    else:\n        predictions = pred.predictions\n        \n    preds = predictions.argmax(-1)\n\n    prec, rec, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted', zero_division=0)\n    acc = accuracy_score(labels, preds)\n    \n    return {'accuracy': acc, 'f1_weighted': f1, 'precision': prec, 'recall': rec}\n\n# ğŸš¨ CHANGE: Pass feature counts as 0, 0 ğŸš¨\nmodel = MultimodalClassifier(N_CLASSES, num_numerical_features, num_categorical_features)\n\nOUTPUT_DIR = \"/kaggle/working/Model_Results_3Class_Weighted_DeBERTaV3_Base_TextOnly_C2\" \n\ntraining_args = TrainingArguments(\n    output_dir=OUTPUT_DIR,\n    learning_rate=2e-5,\n    per_device_train_batch_size=16, \n    per_device_eval_batch_size=16,\n    num_train_epochs=3, \n    weight_decay=0.01,\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"f1_weighted\",  \n    greater_is_better=True,\n    save_total_limit=2,\n    report_to=\"none\",\n    logging_steps=100,\n    fp16=True \n)\n\ntrainer = WeightedTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"test\"],\n    data_collator=MultimodalDataCollator(tokenizer),\n    compute_metrics=compute_metrics,\n)\n\nprint(f\"--- Part 4: DeBERTa V3-Base Weighted Training Setup Complete (Text-Only, 3 Epochs) ---\")\nprint(f\"Saving checkpoints to: {OUTPUT_DIR}\")\nprint(f\"Training Batch Size: {training_args.per_device_train_batch_size}\")\ntrainer.train()\nprint(\"--- Training Complete ---\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T09:17:53.731138Z","iopub.execute_input":"2025-11-26T09:17:53.731464Z","iopub.status.idle":"2025-11-26T09:18:05.967958Z","shell.execute_reply.started":"2025-11-26T09:17:53.731440Z","shell.execute_reply":"2025-11-26T09:18:05.966806Z"},"editable":false},"outputs":[{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/371M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc7c322871f94a5c9361b081cbd1b9e6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/371M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"91f235934b65410ba59e353b800a1dbf"}},"metadata":{}},{"name":"stderr","text":"You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","output_type":"stream"},{"name":"stdout","text":"--- Part 4: DeBERTa V3-Base Weighted Training Setup Complete (Text-Only, 3 Epochs) ---\nSaving checkpoints to: /kaggle/working/Model_Results_3Class_Weighted_DeBERTaV3_Base_TextOnly_C2\nTraining Batch Size: 16\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='9' max='14610' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [    9/14610 00:06 < 3:48:47, 1.06 it/s, Epoch 0.00/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_47/1188603428.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Saving checkpoints to: {OUTPUT_DIR}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Training Batch Size: {training_args.per_device_train_batch_size}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"--- Training Complete ---\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2323\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2324\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2325\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2326\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2327\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2616\u001b[0m                 \u001b[0mupdate_step\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2617\u001b[0m                 \u001b[0mnum_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_accumulation_steps\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mupdate_step\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtotal_updates\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mremainder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2618\u001b[0;31m                 \u001b[0mbatch_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_batch_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2619\u001b[0m                 \u001b[0;31m# Store the number of batches for current gradient accumulation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2620\u001b[0m                 \u001b[0;31m# This is used to correctly scale the loss when the last accumulation step has fewer batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mget_batch_samples\u001b[0;34m(self, epoch_iterator, num_batches, device)\u001b[0m\n\u001b[1;32m   5652\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5653\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5654\u001b[0;31m                 \u001b[0mbatch_samples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5655\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5656\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/data_loader.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    575\u001b[0m                 \u001b[0;31m# But we still move it to the device so it is done before `StopIteration` is reached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m                     \u001b[0mcurrent_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msend_to_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_non_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0mnext_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36msend_to_device\u001b[0;34m(tensor, device, non_blocking, skip_keys)\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"npu:0\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# .to() doesn't accept non_blocking as kwarg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, device, non_blocking)\u001b[0m\n\u001b[1;32m    836\u001b[0m         \u001b[0;31m# into a HalfTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mis_torch_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 838\u001b[0;31m             self.data = {\n\u001b[0m\u001b[1;32m    839\u001b[0m                 \u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"to\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    837\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mis_torch_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m             self.data = {\n\u001b[0;32m--> 839\u001b[0;31m                 \u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"to\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    840\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m             }\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":4},{"cell_type":"code","source":"# --- PART 5: EVALUATION & SAVING (KAGGLE VERSION) ---\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport shutil # For zipping the model\n\n# ==========================\n# EVALUATE â€¢ REPORT â€¢ PLOTS â€¢ SAVE\n# ==========================\nprint(\"\\n--- Final Evaluation on Test Set ---\")\n\n# 1. PLOT LOSS CURVES\n# Access logs\nlog_history = trainer.state.log_history\ntrain_logs = [log for log in log_history if ('loss' in log and 'eval_loss' not in log)]\neval_logs = [log for log in log_history if ('eval_loss' in log)]\n\nplt.figure(figsize=(12, 5))\n\n# Loss Plot\nplt.subplot(1, 2, 1)\nplt.plot([log.get('epoch', i) for i, log in enumerate(train_logs, 1)], [log['loss'] for log in train_logs], label='Training Loss')\nplt.plot([log.get('epoch', i) for i, log in enumerate(eval_logs, 1)], [log['eval_loss'] for log in eval_logs], label='Validation Loss')\nplt.xlabel('Epoch'); plt.ylabel('Loss'); plt.title('Training vs Validation Loss'); plt.legend()\n\n\n# Accuracy Plot\nplt.subplot(1, 2, 2)\nif len(eval_logs) > 0 and 'eval_accuracy' in eval_logs[0]:\n    plt.plot([log.get('epoch', i) for i, log in enumerate(eval_logs, 1)], [log['eval_accuracy'] for log in eval_logs], label='Validation Accuracy')\n    plt.xlabel('Epoch'); plt.ylabel('Accuracy'); plt.title('Validation Accuracy'); plt.legend()\nplt.tight_layout(); plt.show()\n\n\n# 2. PREDICTIONS (FIXED & SAFE)\nprint(\"Generating predictions...\")\npreds_output = trainer.predict(tokenized_datasets[\"test\"])\n\n# --- SAFETY CHECK START ---\nif isinstance(preds_output.predictions, tuple):\n    logits = preds_output.predictions[0]  # Extract logits if it's a tuple\nelse:\n    logits = preds_output.predictions    # Use directly if it's an array\n# --- SAFETY CHECK END ---\n\npred_labels = np.argmax(logits, axis=-1)\ntrue_labels = np.array(tokenized_datasets[\"test\"][\"label\"])\n\n# 3. CLASSIFICATION REPORT (Updated to 4 decimal places)\nprint(\"\\n--- Classification Report ---\")\nreport_dict = classification_report(true_labels, pred_labels, target_names=class_names, output_dict=True)\n\n# digits=4 for detailed precision\nprint(classification_report(true_labels, pred_labels, target_names=class_names, digits=4))\n\n# 4. CONFUSION MATRIX (Updated to 4 decimal places)\ncm = confusion_matrix(true_labels, pred_labels)\ncm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n\nplt.figure(figsize=(12, 5))\n\n# Counts (Integers)\nplt.subplot(1, 2, 1)\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\nplt.title('Confusion Matrix (Counts)')\nplt.ylabel('True Label')\nplt.xlabel('Predicted Label')\n\n# Normalized (Floats with 4 decimals)\nplt.subplot(1, 2, 2)\nsns.heatmap(cm_norm, annot=True, fmt='.4f', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\nplt.title('Confusion Matrix (Normalized)')\nplt.ylabel('True Label')\nplt.xlabel('Predicted Label')\n\nplt.tight_layout()\nplt.show()\n\n# 5. PER-CLASS BAR CHART\nreport_df = pd.DataFrame(report_dict).transpose()\nreport_df_classes = report_df.loc[[cn for cn in class_names if cn in report_df.index]]\nax = report_df_classes[['precision', 'recall', 'f1-score']].plot(kind='bar', figsize=(10, 6))\nplt.title('Per-Class Metrics (Weighted DeBERTa Model, Text-Only)')\nplt.xlabel('Classes'); plt.ylabel('Score')\nplt.xticks(rotation=0); plt.grid(axis='y', linestyle='--'); plt.legend(loc='lower right')\nplt.show()\n\n# 6. SAVE (KAGGLE SPECIFIC)\n# Kaggle saves to /kaggle/working/\nFINAL_MODEL_DIR = \"Final_Model_3Class_DeBERTaV3_Base_TextOnly_C2\" \nFINAL_MODEL_PATH = f\"/kaggle/working/{FINAL_MODEL_DIR}\"\n\nprint(f\"\\nğŸ’¾ Saving model to {FINAL_MODEL_PATH}...\")\ntrainer.save_model(FINAL_MODEL_PATH)\n\n# --- ZIP FOR DOWNLOAD ---\n# Kaggle makes it hard to download folders. We zip it so you can download 1 file.\nprint(\"ğŸ“¦ Zipping model for easy download...\")\nshutil.make_archive(f\"/kaggle/working/{FINAL_MODEL_DIR}\", 'zip', FINAL_MODEL_PATH)\n\nprint(f\"âœ… DONE! You can now download '{FINAL_MODEL_DIR}.zip' from the 'Output' tab on the right.\")","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null}]}